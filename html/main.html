
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Banditology</title><meta name="generator" content="MATLAB 7.13"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2012-06-08"><meta name="DC.source" content="main.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Banditology</h1><!--introduction--><p>We explore various n-armed bandit algorithms presented in chapter 2 of Sutton and Barto's book <i>Reinforcement Learning: An Introduction.</i></p><p>The n-armed bandit is a basic problem in reinforcement learning which posits the <i>exploration-exploitation</i> tradeoff. We have a slot machine with n levers. Pulling a lever generates a stochastic quantity of reward. Suppose that some levers generate, on average, more reward than others. The goal is to generate the greatest cumulative reward.</p><p>In these examples, the player has no prior knowledge of the levers. As the player makes moves, the player will remember how much reward each lever generates. To succeed, the player must balance <i>exploitation</i>---using her current knowledge of the levers to make the best short-term choice, with <i>exploration</i>---deliberately picking a lever other than the local maximum to check that she is not missing out on potentially greater reward.</p><p>This project compares various algorithms to solve the exploration/exploitation tradeoff.</p><p>Finally, reinforcement learning is well-suited for nonstationary problems. Nonstationary bandits will be explored in a later release.</p><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Make bandits.</a></li><li><a href="#2">Try each algorithm in turn.</a></li><li><a href="#7">General comparison of algorithms.</a></li><li><a href="#8">Compare parameter settings for reinforcement comparison.</a></li><li><a href="#10">Adjust probabilities for low initial rewards.</a></li><li><a href="#11">Adjust probabilities for high initial rewards.</a></li></ul></div><h2>Make bandits.<a name="1"></a></h2><pre class="codeinput">clear
<span class="comment">% Make 2000 bandits with variance 1 and whose means are sampled from a</span>
<span class="comment">% standard normal distribution, as in the book.</span>
N = 2000;
T = 1000;
A = 10;

<span class="keyword">global</span> banditmeans banditoptima
<span class="comment">% We have N bandits, each with A actions. Rows are bandits; columns are</span>
<span class="comment">% actions.</span>
banditmeans = randn(N, A);
<span class="comment">% To evaluate performance, we compare the choices our algorithm made to the</span>
<span class="comment">% optimal choice. Given a bandit (row), we define the optimal choice to be</span>
<span class="comment">% the action (column index) with the highest mean reward.</span>
[~, banditoptima] = max(banditmeans, [], 2);
</pre><h2>Try each algorithm in turn.<a name="2"></a></h2><p>The <img src="main_eq97780.png" alt="$\epsilon$">-greedy algorithm is the simplest method to balance exploration and exploitation. It selects the greedy choice (choice with highest <i>value</i>) with probability <img src="main_eq30391.png" alt="$1 - \epsilon$"> and it selects a random uniform choice with probability <img src="main_eq97780.png" alt="$\epsilon$">. The <i>value</i> of an action <img src="main_eq34276.png" alt="$a$"> is the sample mean of rewards given by <img src="main_eq34276.png" alt="$a$">. By the law of large numbers, the value converges to the mean reward of <img src="main_eq34276.png" alt="$a$">.</p><p>Other than asymptotic convergence of action values, <img src="main_eq97780.png" alt="$\epsilon$">-greedy has few theoretical guarantees. Nevertheless, it is insanely simple and easy to tune, and yields great results in the Gaussian case.</p><pre class="codeinput">eg_01 = make_epsilon_greedy(0.1);
</pre><p>A lower <img src="main_eq97780.png" alt="$\epsilon$"> favors more exploitation to exploration. On the graphs, notice that while the percent optimal curve for <img src="main_eq75867.png" alt="$\epsilon = 0.01$"> is significantly lower than that of for <img src="main_eq46103.png" alt="$\epsilon = 0.1$">, the reward curves are much closer. Thus, while it takes longer for the <img src="main_eq75867.png" alt="$\epsilon = 0.01$"> player to find the global optimum, the <img src="main_eq75867.png" alt="$\epsilon = 0.01$"> makes better use of the local optima that it finds.</p><pre class="codeinput">eg_001 = make_epsilon_greedy(0.01);
</pre><p>A drawback of the <img src="main_eq97780.png" alt="$\epsilon$">-greedy algorithms is that every bad move is drawn with probability <img src="main_eq42730.png" alt="$\epsilon / A$">. This may not be desirable if bad moves are very bad. The softmax algorithm maintains sample mean values like the <img src="main_eq97780.png" alt="$\epsilon$">-greedy algorithm, but it always chooses an action from the <i>Gibbs distribution</i>, where the probability of choosing <img src="main_eq34276.png" alt="$a$"> is</p><p><img src="main_eq72053.png" alt="$$\frac{\exp(Q(a) / \tau)}{\sum_{b=1}^A \exp(Q(b) / \tau)}$$"></p><p>Thus, the probability of an action scales exponentially with its value. The parameter <img src="main_eq89224.png" alt="$\tau$"> represents temperature. As <img src="main_eq03680.png" alt="$\tau \rightarrow \infty$">, softmax approaches uniform selection, while as <img src="main_eq72892.png" alt="$\tau \rightarrow 0$">, softmax approaches greedy selection.</p><p>In practice, <img src="main_eq89224.png" alt="$\tau$"> is much less intuitive than <img src="main_eq97780.png" alt="$\epsilon$"> and is consequently hard to tune. The author had to try several times to get a reasonable result.</p><pre class="codeinput">smax = make_softmax(0.3);
</pre><p>Reinforcement comparison... (todo: write up!)</p><pre class="codeinput">rc_01_01 = make_reinforcement_compare(0.1, 0.1);
</pre><p>Pursuit...</p><pre class="codeinput">purs = make_pursuit(0.01);
</pre><h2>General comparison of algorithms.<a name="7"></a></h2><pre class="codeinput">compare_bandits(T, [], <span class="string">'\epsilon = 0.1'</span>, eg_01, <span class="keyword">...</span>
                       <span class="string">'\epsilon = 0.01'</span>, eg_001, <span class="keyword">...</span>
                       <span class="string">'softmax \tau = 0.3'</span>, smax, <span class="keyword">...</span>
                       <span class="string">'comparison \alpha = 0.1, \beta = 0.1'</span>, rc_01_01, <span class="keyword">...</span>
                       <span class="string">'pursuit \beta = 0.01'</span>, purs);
</pre><pre class="codeoutput">Running \epsilon = 0.1
Took 55.7697 seconds.
Running \epsilon = 0.01
Took 57.5529 seconds.
Running softmax \tau = 0.3
Took 53.7945 seconds.
Running comparison \alpha = 0.1, \beta = 0.1
Took 29.4575 seconds.
Running pursuit \beta = 0.01
Took 94.6367 seconds.
</pre><img vspace="5" hspace="5" src="main_01.png" alt=""> <img vspace="5" hspace="5" src="main_02.png" alt=""> <h2>Compare parameter settings for reinforcement comparison.<a name="8"></a></h2><p>In reinforcement pursuit, <img src="main_eq87919.png" alt="$\alpha$"> controls the update rate of the action values while <img src="main_eq42727.png" alt="$\beta$"> controls the update rate of the reference actions. From the previous graphs, we get good results by setting them equal. What if we set them asymmetrically?</p><pre class="codeinput">rc_001_001 = make_reinforcement_compare(0.01, 0.01);
rc_01_001 = make_reinforcement_compare(0.1, 0.01);
rc_001_01 = make_reinforcement_compare(0.01, 0.1);

compare_bandits(4000, <span class="string">'Reinforcement Comparison Parameters'</span>, <span class="keyword">...</span>
                          <span class="string">'\alpha = 0.01, \beta = 0.01'</span>, rc_001_001, <span class="keyword">...</span>
                          <span class="string">'\alpha = 0.1, \beta = 0.01'</span>, rc_01_001, <span class="keyword">...</span>
                          <span class="string">'\alpha = 0.001, \beta = 0.1'</span>, rc_001_01);
</pre><pre class="codeoutput">Running \alpha = 0.01, \beta = 0.01
Took 117.1285 seconds.
Running \alpha = 0.1, \beta = 0.01
Took 121.144 seconds.
Running \alpha = 0.001, \beta = 0.1
Took 118.0624 seconds.
</pre><img vspace="5" hspace="5" src="main_03.png" alt=""> <img vspace="5" hspace="5" src="main_04.png" alt=""> <p>Reducing <img src="main_eq87919.png" alt="$\alpha$"> slightly hurt our results, as we are not able to update our knowledge of action values as quickly. But the loss is not drastic.</p><p>However, reducing <img src="main_eq42727.png" alt="$\beta$">% significantly decreases convergence speed, regardless of the value of <img src="main_eq87919.png" alt="$\alpha$">. In this case, reference rewards are initialized to zero (a "realistic" as opposed to an optimistic setting). Low <img src="main_eq42727.png" alt="$\beta$"> means a slowly-increasing reference reward. Thus, suboptimal actions will continue to exceed the reference reward for a long time and will continue to be selected. A low <img src="main_eq42727.png" alt="$\beta$"> encourages exploration.</p><p>Here, we set T = 4000 to fully observe the effect of additional exploration. Eventually, the curves for <img src="main_eq85801.png" alt="$\beta = 0.01$"> exceed that of <img src="main_eq95685.png" alt="$\beta = 0.1$">, showing that the additional exploration does eventually pay off.</p><h2>Adjust probabilities for low initial rewards.<a name="10"></a></h2><p>If the initial reward is low, reinforcement comparison can result in insufficient exploration because whichever action first will have substantially higher reward and will be favored regardless of whether it was optimal.</p><p>We can compensate by adding (1 - p(a)) to value updates. This encourages exploration: anytime we hit a low-probability action (i.e. an action with low current value), we automatically increase its value to encourage visiting it again.</p><p>Unfortunately, this decreases performances. I hope this is just a bug... if you can help me find it, many thanks to you!</p><pre class="codeinput">unadj   = make_reinforcement_compare(0.1, 0.1, -5);
adj_001 = make_reinforcement_compare_adjusted_prob(0.1, 0.1, 0.01, -5);
adj_01  = make_reinforcement_compare_adjusted_prob(0.1, 0.1, 0.1, -5);

compare_bandits(T, <span class="string">'Probability Adjustment; initial reference = -5'</span>, <span class="keyword">...</span>
                   <span class="string">'unadjusted'</span>, unadj, <span class="keyword">...</span>
                   <span class="string">'adjusted \gamma = 0.01'</span>, adj_001, <span class="keyword">...</span>
                   <span class="string">'adjusted \gamma = 0.1'</span>, adj_01);
</pre><pre class="codeoutput">Running unadjusted
Took 29.3499 seconds.
Running adjusted \gamma = 0.01
Took 34.1567 seconds.
Running adjusted \gamma = 0.1
Took 34.086 seconds.
</pre><img vspace="5" hspace="5" src="main_05.png" alt=""> <img vspace="5" hspace="5" src="main_06.png" alt=""> <h2>Adjust probabilities for high initial rewards.<a name="11"></a></h2><p>Probability adjustments also hurts performance with optimistic initial values.</p><pre class="codeinput">unadj = make_reinforcement_compare(0.1, 0.1, 5);
adj = make_reinforcement_compare_adjusted_prob(0.1, 0.1, 0.1, 5);

compare_bandits(T, <span class="string">'Probability Adjustment; initial reference = 5'</span>, <span class="keyword">...</span>
                   <span class="string">'unadjusted'</span>, unadj, <span class="keyword">...</span>
                   <span class="string">'adjusted \gamma = 0.1'</span>, adj);
</pre><pre class="codeoutput">Running unadjusted
Took 29.3389 seconds.
Running adjusted \gamma = 0.1
Took 34.2384 seconds.
</pre><img vspace="5" hspace="5" src="main_07.png" alt=""> <img vspace="5" hspace="5" src="main_08.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.13<br></p></div><!--
##### SOURCE BEGIN #####
%% Banditology
% We explore various n-armed bandit algorithms presented in chapter 2 of
% Sutton and Barto's book _Reinforcement Learning: An Introduction._
%
% The n-armed bandit is a basic problem in reinforcement learning which
% posits the _exploration-exploitation_ tradeoff. We have a slot
% machine with n levers. Pulling a lever generates a stochastic quantity
% of reward. Suppose that some levers generate, on average, more reward
% than others. The goal is to generate the greatest cumulative reward.
%
% In these examples, the player has no prior knowledge of the levers. As
% the player makes moves, the player will remember how much reward each
% lever generates. To succeed, the player must balance
% _exploitation_REPLACE_WITH_DASH_DASH-using her current knowledge of the levers to make the
% best short-term choice, with _exploration_REPLACE_WITH_DASH_DASH-deliberately picking a lever
% other than the local maximum to check that she is not missing out on
% potentially greater reward.
%
% This project compares various algorithms to solve the
% exploration/exploitation tradeoff.
%
% Finally, reinforcement learning is well-suited for nonstationary
% problems. Nonstationary bandits will be explored in a later release.

%% Make bandits.
clear
% Make 2000 bandits with variance 1 and whose means are sampled from a
% standard normal distribution, as in the book.
N = 2000;
T = 1000;
A = 10;

global banditmeans banditoptima
% We have N bandits, each with A actions. Rows are bandits; columns are
% actions.
banditmeans = randn(N, A);
% To evaluate performance, we compare the choices our algorithm made to the
% optimal choice. Given a bandit (row), we define the optimal choice to be
% the action (column index) with the highest mean reward.
[~, banditoptima] = max(banditmeans, [], 2);


%% Try each algorithm in turn.
% The $\epsilon$-greedy algorithm is the simplest method to balance
% exploration and exploitation. It selects the greedy choice (choice with
% highest _value_) with probability $1 - \epsilon$ and it selects a
% random uniform choice with probability $\epsilon$. The _value_ of an
% action $a$ is the sample mean of rewards given by $a$. By the law of
% large numbers, the value converges to the mean reward of $a$.
%
% Other than asymptotic convergence of action values, $\epsilon$-greedy has
% few theoretical guarantees. Nevertheless, it is insanely simple and easy
% to tune, and yields great results in the Gaussian case.
eg_01 = make_epsilon_greedy(0.1);

%%
% A lower $\epsilon$ favors more exploitation to exploration. On the
% graphs, notice that while the percent optimal curve for $\epsilon = 0.01$
% is significantly lower than that of for $\epsilon = 0.1$, the reward
% curves are much closer. Thus, while it takes longer for the $\epsilon =
% 0.01$ player to find the global optimum, the $\epsilon = 0.01$ makes
% better use of the local optima that it finds.
eg_001 = make_epsilon_greedy(0.01);

%%
% A drawback of the $\epsilon$-greedy algorithms is that every bad move is
% drawn with probability $\epsilon / A$. This may not be desirable if bad
% moves are very bad. The softmax algorithm maintains sample mean values
% like the $\epsilon$-greedy algorithm, but it always chooses an action
% from the _Gibbs distribution_, where the probability of choosing $a$ is
%
% $$\frac{\exp(Q(a) / \tau)}{\sum_{b=1}^A \exp(Q(b) / \tau)}$$
%
% Thus, the probability of an action scales exponentially with its value.
% The parameter $\tau$ represents temperature. As $\tau \rightarrow
% \infty$, softmax approaches uniform selection, while as $\tau \rightarrow
% 0$, softmax approaches greedy selection.
%
% In practice, $\tau$ is much less intuitive than $\epsilon$ and is
% consequently hard to tune. The author had to try several times to get a
% reasonable result.
smax = make_softmax(0.3);

%%
% Reinforcement comparison... (todo: write up!)
rc_01_01 = make_reinforcement_compare(0.1, 0.1);

%%
% Pursuit...
purs = make_pursuit(0.01);

%% General comparison of algorithms.
compare_bandits(T, [], '\epsilon = 0.1', eg_01, ...
                       '\epsilon = 0.01', eg_001, ...
                       'softmax \tau = 0.3', smax, ...
                       'comparison \alpha = 0.1, \beta = 0.1', rc_01_01, ...
                       'pursuit \beta = 0.01', purs);

%% Compare parameter settings for reinforcement comparison.
% In reinforcement pursuit, $\alpha$ controls the update rate of the action
% values while $\beta$ controls the update rate of the reference actions.
% From the previous graphs, we get good results by setting them equal. What
% if we set them asymmetrically?
rc_001_001 = make_reinforcement_compare(0.01, 0.01);
rc_01_001 = make_reinforcement_compare(0.1, 0.01);
rc_001_01 = make_reinforcement_compare(0.01, 0.1);

compare_bandits(4000, 'Reinforcement Comparison Parameters', ...
                          '\alpha = 0.01, \beta = 0.01', rc_001_001, ...
                          '\alpha = 0.1, \beta = 0.01', rc_01_001, ...
                          '\alpha = 0.001, \beta = 0.1', rc_001_01);
%%
% Reducing $\alpha$ slightly hurt our results, as we are not able to update
% our knowledge of action values as quickly. But the loss is not drastic.
%
% However, reducing $\beta$% significantly decreases convergence speed,
% regardless of the value of $\alpha$. In this case, reference rewards are
% initialized to zero (a "realistic" as opposed to an optimistic setting).
% Low $\beta$ means a slowly-increasing reference reward. Thus, suboptimal
% actions will continue to exceed the reference reward for a long time and
% will continue to be selected. A low $\beta$ encourages exploration.
%
% Here, we set T = 4000 to fully observe the effect of additional
% exploration. Eventually, the curves for $\beta = 0.01$ exceed that of
% $\beta = 0.1$, showing that the additional exploration does eventually
% pay off.

%% Adjust probabilities for low initial rewards.
% If the initial reward is low, reinforcement comparison can result in
% insufficient exploration because whichever action first will have
% substantially higher reward and will be favored regardless of whether it
% was optimal.
%
% We can compensate by adding (1 - p(a)) to value updates. This encourages
% exploration: anytime we hit a low-probability action (i.e. an action with
% low current value), we automatically increase its value to encourage
% visiting it again.
%
% Unfortunately, this decreases performances. I hope this is just a bug...
% if you can help me find it, many thanks to you!
unadj   = make_reinforcement_compare(0.1, 0.1, -5);
adj_001 = make_reinforcement_compare_adjusted_prob(0.1, 0.1, 0.01, -5);
adj_01  = make_reinforcement_compare_adjusted_prob(0.1, 0.1, 0.1, -5);

compare_bandits(T, 'Probability Adjustment; initial reference = -5', ...
                   'unadjusted', unadj, ...
                   'adjusted \gamma = 0.01', adj_001, ...
                   'adjusted \gamma = 0.1', adj_01);

%% Adjust probabilities for high initial rewards.
% Probability adjustments also hurts performance with optimistic initial
% values.
unadj = make_reinforcement_compare(0.1, 0.1, 5);
adj = make_reinforcement_compare_adjusted_prob(0.1, 0.1, 0.1, 5);

compare_bandits(T, 'Probability Adjustment; initial reference = 5', ...
                   'unadjusted', unadj, ...
                   'adjusted \gamma = 0.1', adj);

##### SOURCE END #####
--></body></html>